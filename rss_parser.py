import feedparser
from bs4 import BeautifulSoup
import re
import datetime
import time
import os
import requests
import json
import io
import pypdf
import traceback

from rank_bm25 import BM25Okapi

from google import genai  # âœ… ë³€ê²½ëœ import

# --- API í‚¤ ì„¤ì • ---
OPENROUTER_API_KEY = os.getenv('OPENROUTER_API_KEY')
DISCORD_WEBHOOK_URL = os.getenv('DISCORD_WEBHOOK_URL')
GEMINI_API_KEY = os.getenv('GEMINI_API_KEY')

# --- Gemini Client ì„¤ì • (ì‹ ê·œ SDK) ---
gemini_client = None
if GEMINI_API_KEY:
    try:
        # âœ… ì‹ ê·œ SDK ì´ˆê¸°í™” ë°©ì‹
        gemini_client = genai.Client(api_key=GEMINI_API_KEY)
        print("âœ… Gemini Client ì„¤ì • ì™„ë£Œ")
    except Exception as e:
        print(f"ğŸš¨ Gemini Client ì„¤ì • ì‹¤íŒ¨: {e}")

# --- ì‚¬ìš©ì ê´€ì‹¬ì‚¬ ì •ì˜ ---
USER_INTERESTS = """
- Embedding Model, Reranker, multi-vector, col-bert, Information Retriever 
- LLM, LLM Quantization
- LoRA fine-tuning, Domain-Adaptation, Continual-Learning
- sparse vector, dense vector, vector DB & Search, indexing, ANN
- Retrieval-Augmented Generation (RAG)
"""

def filter_papers_bm25(papers, top_k=25):
    """
    BM25ë¥¼ ì‚¬ìš©í•˜ì—¬ ê´€ì‹¬ì‚¬ì™€ ìœ ì‚¬ë„ê°€ ë†’ì€ ë…¼ë¬¸ì„ 1ì°¨ì ìœ¼ë¡œ ì„ ë³„
    """
    if not papers: return []
    
    # 1. ê´€ì‹¬ì‚¬ í‚¤ì›Œë“œ í† í°í™” (ê°„ë‹¨í•œ ì†Œë¬¸ì ê³µë°± ë¶„ë¦¬)
    query = re.sub(r'[^\w\s]', '', USER_INTERESTS.lower()).split()
    
    # 2. ë…¼ë¬¸ Abstract í† í°í™”
    corpus = []
    for paper in papers:
        summary = BeautifulSoup(paper.summary, 'html.parser').get_text(separator=" ", strip=True)
        # íŠ¹ìˆ˜ë¬¸ì ì œê±° ë° ì†Œë¬¸ìí™”í•˜ì—¬ í† í° ìƒì„±
        tokens = re.sub(r'[^\w\s]', '', summary.lower()).split()
        corpus.append(tokens)
    
    # 3. BM25 ëª¨ë¸ ìƒì„± ë° ì ìˆ˜ ê³„ì‚°
    bm25 = BM25Okapi(corpus)
    doc_scores = bm25.get_scores(query)
    
    # 4. ì ìˆ˜ì™€ í•¨ê»˜ ë…¼ë¬¸ ì €ì¥ í›„ ì •ë ¬
    scored_papers = list(zip(papers, doc_scores))
    scored_papers.sort(key=lambda x: x[1], reverse=True)
    
    # ìƒìœ„ top_kê°œ ë°˜í™˜
    selected = [p[0] for p in scored_papers[:top_k] if p[1] > 0] # ì ìˆ˜ê°€ 0ì¸ ê²ƒì€ ì œì™¸
    print(f"ğŸ” BM25 í•„í„°ë§: {len(papers)}ê°œ ì¤‘ {len(selected)}ê°œ ì„ ë³„ (Top {top_k})")
    return selected



def get_paper_relevance_scores_openrouter(papers_batch):
    """
    OpenRouter API (Reasoning) - ê¸°ì¡´ ë¡œì§ ìœ ì§€
    """
    if not papers_batch: return []
    if not OPENROUTER_API_KEY:
        print("ğŸš¨ OpenRouter API Keyê°€ ì—†ìŠµë‹ˆë‹¤.")
        return []

    prompt_papers_section = ""
    for i, paper in enumerate(papers_batch):
        title = re.sub(r'\s*\(v\d+\)$', '', paper.title)
        summary = BeautifulSoup(paper.summary, 'html.parser').get_text(separator=" ", strip=True)
        prompt_papers_section += f'\n{{ "id": {i}, "title": "{title}", "abstract": "{summary[:900]}" }}'

    system_prompt = "You are an expert AI researcher. Analyze the papers carefully based on the user's interests."
    
    user_prompt = f"""
    Evaluate the relevance of the following papers based on my interests.
    
    --- My Interests ---
    {USER_INTERESTS}
    --------------------

    **Reasoning Task:**
    1. Think step-by-step about how each paper's abstract matches the specific technical keywords in my interests.
    2. Assign a relevance score from 0 to 100.
    3. Exclude papers focusing on specific languages (Thai, Arabic, etc.) unless it's Korean.

    --- Papers to Evaluate ---
    [
        {prompt_papers_section}
    ]
    --------------------

    **Output Format:**
    Provide the output ONLY as a valid JSON list of objects. Do not include markdown code blocks.
    Example: [ {{"id": 0, "score": 15}}, {{"id": 1, "score": 95}} ]
    """

    headers = {
        "Authorization": f"Bearer {OPENROUTER_API_KEY}",
        "Content-Type": "application/json",
    }

    payload = {
        "model": "nvidia/nemotron-3-nano-30b-a3b:free", 
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ],
        # "reasoning": {"enabled": True}, 
        # "temperature": 0.2 
    }

    try:
        response = requests.post(
            url="https://openrouter.ai/api/v1/chat/completions",
            headers=headers,
            data=json.dumps(payload),
            timeout=60
        )
        response.raise_for_status()
        
        result_json = response.json()
        choice = result_json['choices'][0]
        content_text = choice['message']['content']

        clean_json_text = content_text.replace("```json", "").replace("```", "").strip()
        scores_data = json.loads(clean_json_text)

        scored_papers = []
        scores_map = {item['id']: item['score'] for item in scores_data}

        for i, paper in enumerate(papers_batch):
            score = scores_map.get(i, 0)
            if score >= 75: 
                scored_papers.append({'paper': paper, 'score': score})
        
        print(f"   - OpenRouter ë¶„ì„ ì™„ë£Œ: {len(papers_batch)}ê°œ ì¤‘ {len(scored_papers)}ê°œ ì„ ì •.")
        return scored_papers

    except Exception as e:
        print(f"   - â— OpenRouter API í˜¸ì¶œ/íŒŒì‹± ì—ëŸ¬: {e}")
        traceback.print_exc()
        return []

def summarize_paper_gemini(paper_url):
    """
    âœ… Gemini ì‹ ê·œ SDK ì ìš© (google-genai)
    """
    if not gemini_client: return "Gemini Client ë¯¸ì„¤ì •ìœ¼ë¡œ ìš”ì•½ ë¶ˆê°€"
    
    try:
        pdf_url = paper_url.replace('/abs/', '/pdf/')
        res = requests.get(pdf_url, headers={'User-Agent': 'Mozilla/5.0'}, timeout=30)
        pdf_file = io.BytesIO(res.content)
        reader = pypdf.PdfReader(pdf_file)
        # ì• 9í˜ì´ì§€ ì¶”ì¶œ
        text = "".join([page.extract_text() or "" for page in reader.pages[:9]])

        # ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
        cleaned_text = re.sub(r'\s+', ' ', text).strip()
        if not cleaned_text:
            return "PDFì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        prompt = f"""
        You are a research assistant. Read the following excerpt from a research paper and provide a structured summary in KOREAN. 
        The summary must follow this structure:
        - **1. 3-line Summary**
        - **2. Problem Statement**
        - **3. Proposed Method**
        - **4. Key Contribution**
        - **5. Results & Evaluation**
        
        --- Paper Excerpt ---
        Text: {cleaned_text[:12000]} 
        --- End of Excerpt ---

        Your structured summary in Korean:
        """
        
        # âœ… ë³€ê²½ëœ í˜¸ì¶œ ë°©ì‹: client.models.generate_content
        # ëª¨ë¸ëª…ì€ 'gemini-2.0-flash' (ìµœì‹ ) í˜¹ì€ 'gemini-1.5-flash' ì‚¬ìš© ê¶Œì¥
        response = gemini_client.models.generate_content(
            model='gemini-2.5-flash', 
            contents=prompt
        )
        
        return response.text.strip()
        
    except Exception as e:
        return f"ìš”ì•½ ì‹¤íŒ¨: {e}"

def send_discord_briefing(papers_list, category_name):
    if not DISCORD_WEBHOOK_URL: return
    
    today_str = datetime.date.today().strftime("%Y-%m-%d")
    header = {"content": f"## ğŸ§  {today_str} **{category_name}** ë…¼ë¬¸ (Reasoning Filtered)"}
    requests.post(DISCORD_WEBHOOK_URL, json=header)

    for item in papers_list:
        paper = item['paper']
        embed = {
            "title": f"ğŸ“„ {paper.title[:200]}",
            "url": paper.link,
            "description": item.get('summary', 'ìš”ì•½ ì—†ìŒ')[:2000],
            "color": 3447003, 
            "fields": [
                {"name": "Score", "value": f"**{item['score']}** / 100", "inline": True},
                {"name": "Published", "value": paper.published.split('T')[0], "inline": True}
            ]
        }
        requests.post(DISCORD_WEBHOOK_URL, json={"embeds": [embed]})
        time.sleep(1)

# --- ë©”ì¸ ì‹¤í–‰ ë¡œì§ ---
arxiv_urls = ["http://export.arxiv.org/rss/cs.AI", "http://export.arxiv.org/rss/cs.CL"]
yesterday = datetime.date.today() - datetime.timedelta(days=1)

print(f"ğŸ“… ê¸°ì¤€ ë‚ ì§œ: {yesterday}")

for url in arxiv_urls:
    category = url.split('/')[-1]
    print(f"\n[{category}] ìˆ˜ì§‘ ì‹œì‘...")
    
    feed = feedparser.parse(url)
    recent_papers = [
        e for e in feed.entries 
        if datetime.date(e.published_parsed.tm_year, e.published_parsed.tm_mon, e.published_parsed.tm_mday) >= yesterday
    ]

    if not recent_papers:
        print(" -> ìƒˆ ë…¼ë¬¸ ì—†ìŒ.")
        continue

    # LLMì— ë³´ë‚´ê¸° ì „ BM25ë¡œ 1ì°¨ í•„í„°ë§ (ì˜ˆ: ìƒìœ„ top_kê°œë§Œ ë‚¨ê¹€)
    filtered_by_bm25 = filter_papers_bm25(recent_papers, top_k=32)

    if not filtered_by_bm25:
        print(" -> BM25 ê¸°ì¤€ì„ í†µê³¼í•œ ë…¼ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤.")
        continue

    # ë°°ì¹˜ ì²˜ë¦¬ (OpenRouter Reasoning ì‚¬ìš©)
    top_papers = []
    batch_size = 8 
    for i in range(0, len(filtered_by_bm25), batch_size):
        batch = filtered_by_bm25[i:i+batch_size]
        print(f" -> ë°°ì¹˜ {i//batch_size + 1} í‰ê°€ ì¤‘ (OpenRouter)...")
        scores = get_paper_relevance_scores_openrouter(batch)
        top_papers.extend(scores)
        time.sleep(20) 
    
    if len(top_papers) == 0:
        raise ValueError('í‰ê°€ëœ ë…¼ë¬¸ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. í‰ê°€ ë‹¨ê³„ ë° APIë¥¼ í™•ì¸í•˜ì„¸ìš”.')

    # ìƒìœ„ 8ê°œ ì„ ì • ë° ìš”ì•½
    top_papers.sort(key=lambda x: x['score'], reverse=True)
    final_list = top_papers[:8]

    for item in final_list:
        print(f" -> ìš”ì•½ ìƒì„± ì¤‘: {item['paper'].title[:30]}...")
        item['summary'] = summarize_paper_gemini(item['paper'].link)
        time.sleep(20)

    if final_list:
        send_discord_briefing(final_list, category)

print("\nì™„ë£Œ.")
